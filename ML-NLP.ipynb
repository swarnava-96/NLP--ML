{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e039251",
   "metadata": {},
   "source": [
    "#  NLP using Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cefdb7",
   "metadata": {},
   "source": [
    "### Topics Covered:\n",
    "\n",
    "##### 1)Tokenizing sentences\n",
    "##### 2)Tokenizing words\n",
    "##### 3)Stemming\n",
    "##### 4)Lemmatization\n",
    "##### 5)Bag of Words\n",
    "##### 6)Term Frequency--Inverse Document Frequency(TF-IDF)\n",
    "##### 7)Word2vVec\n",
    "##### 8)TextBlob\n",
    "##### 9)Texthero\n",
    "##### 10) EvalML\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a6e39a",
   "metadata": {},
   "source": [
    "#  Tokenization of paragraphs/sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "914c72ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be82ba2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6543289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4477b8c1",
   "metadata": {},
   "source": [
    "## Tokenizing sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fd9e73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bcd4a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I have three visions for India.',\n",
       " 'In 3000 years of our history, people from all over \\n               the world have come and invaded us, captured our lands, conquered our minds.',\n",
       " 'From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\\n               the French, the Dutch, all of them came and looted us, took over what was ours.',\n",
       " 'Yet we have not done this to any other nation.',\n",
       " 'We have not conquered anyone.',\n",
       " 'We have not grabbed their land, their culture, \\n               their history and tried to enforce our way of life on them.',\n",
       " 'Why?',\n",
       " 'Because we respect the freedom of others.That is why my \\n               first vision is that of freedom.',\n",
       " 'I believe that India got its first vision of \\n               this in 1857, when we started the War of Independence.',\n",
       " 'It is this freedom that\\n               we must protect and nurture and build on.',\n",
       " 'If we are not free, no one will respect us.',\n",
       " 'My second vision for India’s development.',\n",
       " 'For fifty years we have been a developing nation.',\n",
       " 'It is time we see ourselves as a developed nation.',\n",
       " 'We are among the top 5 nations of the world\\n               in terms of GDP.',\n",
       " 'We have a 10 percent growth rate in most areas.',\n",
       " 'Our poverty levels are falling.',\n",
       " 'Our achievements are being globally recognised today.',\n",
       " 'Yet we lack the self-confidence to\\n               see ourselves as a developed nation, self-reliant and self-assured.',\n",
       " 'Isn’t this incorrect?',\n",
       " 'I have a third vision.',\n",
       " 'India must stand up to the world.',\n",
       " 'Because I believe that unless India \\n               stands up to the world, no one will respect us.',\n",
       " 'Only strength respects strength.',\n",
       " 'We must be \\n               strong not only as a military power but also as an economic power.',\n",
       " 'Both must go hand-in-hand.',\n",
       " 'My good fortune was to have worked with three great minds.',\n",
       " 'Dr. Vikram Sarabhai of the Dept.',\n",
       " 'of \\n               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.',\n",
       " 'I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.',\n",
       " 'I see four milestones in my career']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78a3d8e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b6b148",
   "metadata": {},
   "source": [
    "##  Tokenizing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d9cf7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb3c77ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'have',\n",
       " 'three',\n",
       " 'visions',\n",
       " 'for',\n",
       " 'India',\n",
       " '.',\n",
       " 'In',\n",
       " '3000',\n",
       " 'years',\n",
       " 'of',\n",
       " 'our',\n",
       " 'history',\n",
       " ',',\n",
       " 'people',\n",
       " 'from',\n",
       " 'all',\n",
       " 'over',\n",
       " 'the',\n",
       " 'world',\n",
       " 'have',\n",
       " 'come',\n",
       " 'and',\n",
       " 'invaded',\n",
       " 'us',\n",
       " ',',\n",
       " 'captured',\n",
       " 'our',\n",
       " 'lands',\n",
       " ',',\n",
       " 'conquered',\n",
       " 'our',\n",
       " 'minds',\n",
       " '.',\n",
       " 'From',\n",
       " 'Alexander',\n",
       " 'onwards',\n",
       " ',',\n",
       " 'the',\n",
       " 'Greeks',\n",
       " ',',\n",
       " 'the',\n",
       " 'Turks',\n",
       " ',',\n",
       " 'the',\n",
       " 'Moguls',\n",
       " ',',\n",
       " 'the',\n",
       " 'Portuguese',\n",
       " ',',\n",
       " 'the',\n",
       " 'British',\n",
       " ',',\n",
       " 'the',\n",
       " 'French',\n",
       " ',',\n",
       " 'the',\n",
       " 'Dutch',\n",
       " ',',\n",
       " 'all',\n",
       " 'of',\n",
       " 'them',\n",
       " 'came',\n",
       " 'and',\n",
       " 'looted',\n",
       " 'us',\n",
       " ',',\n",
       " 'took',\n",
       " 'over',\n",
       " 'what',\n",
       " 'was',\n",
       " 'ours',\n",
       " '.',\n",
       " 'Yet',\n",
       " 'we',\n",
       " 'have',\n",
       " 'not',\n",
       " 'done',\n",
       " 'this',\n",
       " 'to',\n",
       " 'any',\n",
       " 'other',\n",
       " 'nation',\n",
       " '.',\n",
       " 'We',\n",
       " 'have',\n",
       " 'not',\n",
       " 'conquered',\n",
       " 'anyone',\n",
       " '.',\n",
       " 'We',\n",
       " 'have',\n",
       " 'not',\n",
       " 'grabbed',\n",
       " 'their',\n",
       " 'land',\n",
       " ',',\n",
       " 'their',\n",
       " 'culture',\n",
       " ',',\n",
       " 'their',\n",
       " 'history',\n",
       " 'and',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'enforce',\n",
       " 'our',\n",
       " 'way',\n",
       " 'of',\n",
       " 'life',\n",
       " 'on',\n",
       " 'them',\n",
       " '.',\n",
       " 'Why',\n",
       " '?',\n",
       " 'Because',\n",
       " 'we',\n",
       " 'respect',\n",
       " 'the',\n",
       " 'freedom',\n",
       " 'of',\n",
       " 'others.That',\n",
       " 'is',\n",
       " 'why',\n",
       " 'my',\n",
       " 'first',\n",
       " 'vision',\n",
       " 'is',\n",
       " 'that',\n",
       " 'of',\n",
       " 'freedom',\n",
       " '.',\n",
       " 'I',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'India',\n",
       " 'got',\n",
       " 'its',\n",
       " 'first',\n",
       " 'vision',\n",
       " 'of',\n",
       " 'this',\n",
       " 'in',\n",
       " '1857',\n",
       " ',',\n",
       " 'when',\n",
       " 'we',\n",
       " 'started',\n",
       " 'the',\n",
       " 'War',\n",
       " 'of',\n",
       " 'Independence',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'this',\n",
       " 'freedom',\n",
       " 'that',\n",
       " 'we',\n",
       " 'must',\n",
       " 'protect',\n",
       " 'and',\n",
       " 'nurture',\n",
       " 'and',\n",
       " 'build',\n",
       " 'on',\n",
       " '.',\n",
       " 'If',\n",
       " 'we',\n",
       " 'are',\n",
       " 'not',\n",
       " 'free',\n",
       " ',',\n",
       " 'no',\n",
       " 'one',\n",
       " 'will',\n",
       " 'respect',\n",
       " 'us',\n",
       " '.',\n",
       " 'My',\n",
       " 'second',\n",
       " 'vision',\n",
       " 'for',\n",
       " 'India',\n",
       " '’',\n",
       " 's',\n",
       " 'development',\n",
       " '.',\n",
       " 'For',\n",
       " 'fifty',\n",
       " 'years',\n",
       " 'we',\n",
       " 'have',\n",
       " 'been',\n",
       " 'a',\n",
       " 'developing',\n",
       " 'nation',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'time',\n",
       " 'we',\n",
       " 'see',\n",
       " 'ourselves',\n",
       " 'as',\n",
       " 'a',\n",
       " 'developed',\n",
       " 'nation',\n",
       " '.',\n",
       " 'We',\n",
       " 'are',\n",
       " 'among',\n",
       " 'the',\n",
       " 'top',\n",
       " '5',\n",
       " 'nations',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'GDP',\n",
       " '.',\n",
       " 'We',\n",
       " 'have',\n",
       " 'a',\n",
       " '10',\n",
       " 'percent',\n",
       " 'growth',\n",
       " 'rate',\n",
       " 'in',\n",
       " 'most',\n",
       " 'areas',\n",
       " '.',\n",
       " 'Our',\n",
       " 'poverty',\n",
       " 'levels',\n",
       " 'are',\n",
       " 'falling',\n",
       " '.',\n",
       " 'Our',\n",
       " 'achievements',\n",
       " 'are',\n",
       " 'being',\n",
       " 'globally',\n",
       " 'recognised',\n",
       " 'today',\n",
       " '.',\n",
       " 'Yet',\n",
       " 'we',\n",
       " 'lack',\n",
       " 'the',\n",
       " 'self-confidence',\n",
       " 'to',\n",
       " 'see',\n",
       " 'ourselves',\n",
       " 'as',\n",
       " 'a',\n",
       " 'developed',\n",
       " 'nation',\n",
       " ',',\n",
       " 'self-reliant',\n",
       " 'and',\n",
       " 'self-assured',\n",
       " '.',\n",
       " 'Isn',\n",
       " '’',\n",
       " 't',\n",
       " 'this',\n",
       " 'incorrect',\n",
       " '?',\n",
       " 'I',\n",
       " 'have',\n",
       " 'a',\n",
       " 'third',\n",
       " 'vision',\n",
       " '.',\n",
       " 'India',\n",
       " 'must',\n",
       " 'stand',\n",
       " 'up',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " 'Because',\n",
       " 'I',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'unless',\n",
       " 'India',\n",
       " 'stands',\n",
       " 'up',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " ',',\n",
       " 'no',\n",
       " 'one',\n",
       " 'will',\n",
       " 'respect',\n",
       " 'us',\n",
       " '.',\n",
       " 'Only',\n",
       " 'strength',\n",
       " 'respects',\n",
       " 'strength',\n",
       " '.',\n",
       " 'We',\n",
       " 'must',\n",
       " 'be',\n",
       " 'strong',\n",
       " 'not',\n",
       " 'only',\n",
       " 'as',\n",
       " 'a',\n",
       " 'military',\n",
       " 'power',\n",
       " 'but',\n",
       " 'also',\n",
       " 'as',\n",
       " 'an',\n",
       " 'economic',\n",
       " 'power',\n",
       " '.',\n",
       " 'Both',\n",
       " 'must',\n",
       " 'go',\n",
       " 'hand-in-hand',\n",
       " '.',\n",
       " 'My',\n",
       " 'good',\n",
       " 'fortune',\n",
       " 'was',\n",
       " 'to',\n",
       " 'have',\n",
       " 'worked',\n",
       " 'with',\n",
       " 'three',\n",
       " 'great',\n",
       " 'minds',\n",
       " '.',\n",
       " 'Dr.',\n",
       " 'Vikram',\n",
       " 'Sarabhai',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Dept',\n",
       " '.',\n",
       " 'of',\n",
       " 'space',\n",
       " ',',\n",
       " 'Professor',\n",
       " 'Satish',\n",
       " 'Dhawan',\n",
       " ',',\n",
       " 'who',\n",
       " 'succeeded',\n",
       " 'him',\n",
       " 'and',\n",
       " 'Dr.',\n",
       " 'Brahm',\n",
       " 'Prakash',\n",
       " ',',\n",
       " 'father',\n",
       " 'of',\n",
       " 'nuclear',\n",
       " 'material',\n",
       " '.',\n",
       " 'I',\n",
       " 'was',\n",
       " 'lucky',\n",
       " 'to',\n",
       " 'have',\n",
       " 'worked',\n",
       " 'with',\n",
       " 'all',\n",
       " 'three',\n",
       " 'of',\n",
       " 'them',\n",
       " 'closely',\n",
       " 'and',\n",
       " 'consider',\n",
       " 'this',\n",
       " 'the',\n",
       " 'great',\n",
       " 'opportunity',\n",
       " 'of',\n",
       " 'my',\n",
       " 'life',\n",
       " '.',\n",
       " 'I',\n",
       " 'see',\n",
       " 'four',\n",
       " 'milestones',\n",
       " 'in',\n",
       " 'my',\n",
       " 'career']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17e5bc43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8639c7b5",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cc2ca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14118cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36ace503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36cd85d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "716bb11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c047950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i three vision india .',\n",
       " 'in 3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
       " 'from alexand onward , greek , turk , mogul , portugues , british , french , dutch , came loot us , took .',\n",
       " 'yet done nation .',\n",
       " 'we conquer anyon .',\n",
       " 'we grab land , cultur , histori tri enforc way life .',\n",
       " 'whi ?',\n",
       " 'becaus respect freedom others.that first vision freedom .',\n",
       " 'i believ india got first vision 1857 , start war independ .',\n",
       " 'it freedom must protect nurtur build .',\n",
       " 'if free , one respect us .',\n",
       " 'my second vision india ’ develop .',\n",
       " 'for fifti year develop nation .',\n",
       " 'it time see develop nation .',\n",
       " 'we among top 5 nation world term gdp .',\n",
       " 'we 10 percent growth rate area .',\n",
       " 'our poverti level fall .',\n",
       " 'our achiev global recognis today .',\n",
       " 'yet lack self-confid see develop nation , self-reli self-assur .',\n",
       " 'isn ’ incorrect ?',\n",
       " 'i third vision .',\n",
       " 'india must stand world .',\n",
       " 'becaus i believ unless india stand world , one respect us .',\n",
       " 'onli strength respect strength .',\n",
       " 'we must strong militari power also econom power .',\n",
       " 'both must go hand-in-hand .',\n",
       " 'my good fortun work three great mind .',\n",
       " 'dr. vikram sarabhai dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
       " 'i lucki work three close consid great opportun life .',\n",
       " 'i see four mileston career']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18fb51c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d23435",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9dd72b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9c14afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b026924",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "640f9341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I three vision India .',\n",
       " 'In 3000 year history , people world come invaded u , captured land , conquered mind .',\n",
       " 'From Alexander onwards , Greeks , Turks , Moguls , Portuguese , British , French , Dutch , came looted u , took .',\n",
       " 'Yet done nation .',\n",
       " 'We conquered anyone .',\n",
       " 'We grabbed land , culture , history tried enforce way life .',\n",
       " 'Why ?',\n",
       " 'Because respect freedom others.That first vision freedom .',\n",
       " 'I believe India got first vision 1857 , started War Independence .',\n",
       " 'It freedom must protect nurture build .',\n",
       " 'If free , one respect u .',\n",
       " 'My second vision India ’ development .',\n",
       " 'For fifty year developing nation .',\n",
       " 'It time see developed nation .',\n",
       " 'We among top 5 nation world term GDP .',\n",
       " 'We 10 percent growth rate area .',\n",
       " 'Our poverty level falling .',\n",
       " 'Our achievement globally recognised today .',\n",
       " 'Yet lack self-confidence see developed nation , self-reliant self-assured .',\n",
       " 'Isn ’ incorrect ?',\n",
       " 'I third vision .',\n",
       " 'India must stand world .',\n",
       " 'Because I believe unless India stand world , one respect u .',\n",
       " 'Only strength respect strength .',\n",
       " 'We must strong military power also economic power .',\n",
       " 'Both must go hand-in-hand .',\n",
       " 'My good fortune worked three great mind .',\n",
       " 'Dr. Vikram Sarabhai Dept .',\n",
       " 'space , Professor Satish Dhawan , succeeded Dr. Brahm Prakash , father nuclear material .',\n",
       " 'I lucky worked three closely consider great opportunity life .',\n",
       " 'I see four milestone career']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f155de2",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7264c8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the texts\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7fe81dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "12bdbd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b2324ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    review=re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    review=[ps.stem(word)for word in review if not word in set(stopwords.words('english'))]\n",
    "    review=' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8284751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(max_features=1500)\n",
    "X=cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "42fc28e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5213334",
   "metadata": {},
   "source": [
    "## Term Frequency--Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb93837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the texts\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1568f3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet=WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "051321a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    review=re.sub('[^a-zA-Z]',' ', sentences[i])\n",
    "    review =review.lower()\n",
    "    review=review.split()\n",
    "    review= [wordnet.lemmatize(word)for word in review if not word in set(stopwords.words('english'))]\n",
    "    review=' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41f79cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the TF-IDF model\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67038761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.25883507, 0.30512561,\n",
       "        0.        ],\n",
       "       [0.        , 0.28867513, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8a948d",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74e4dbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SWARNAVA\\anaconda3\\envs\\nlp\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32e7dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80c45869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data\n",
    "\n",
    "text=re.sub(r'\\[[0-9]*\\]',' ', paragraph)\n",
    "text=re.sub(r'\\s+',' ',text)\n",
    "text=text.lower()\n",
    "text=re.sub(r'\\d',' ',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "136b7dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the dataset\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc2e2567",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[nltk.word_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a0acd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    sentences[i]=[word for word in sentences[i] if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9f72ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Training the Word2Vec model\n",
    "\n",
    "model = Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6b65b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Word Vectors\n",
    "\n",
    "vector = model.wv['war']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73f0609b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most similar words\n",
    "similar = model.wv.most_similar('vikram')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197fa94d",
   "metadata": {},
   "source": [
    "## TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "677a5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6861bb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=TextBlob('I havv gooood speling!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdd1739f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"I have good spelling!\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.correct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c06859",
   "metadata": {},
   "source": [
    "https://textblob.readthedocs.io/en/dev/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d7f106",
   "metadata": {},
   "source": [
    "## Texthero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd607e2",
   "metadata": {},
   "source": [
    "#### Under the hoods, Texthero makes use of multiple NLP and machine learning toolkits such as Gensim, NLTK, SpaCy and scikit- learn. You don't need to install them all separately, pip will take care of that.\n",
    "\n",
    "### Texthero include tools for:\n",
    "\n",
    "##### 1. Preprocess text data: it offers both out-of-the-box solutions but it's also flexible for custom-solutions.\n",
    "##### 2. Natural Language Processing: keyphrases and keywords extraction, and named entity recognition.\n",
    "##### 3. Text representation: TF-IDF, term frequency, and custom word-embeddings (wip)\n",
    "##### 4. Vector space analysis: clustering (K-means, Meanshift, DBSAN and Hierarchical), topic modelling (wip) and interpretation.\n",
    "##### 5. Text visualization: vector space visualization, place localization on maps (wip).\n",
    "\n",
    "### Supported representation algorithms:\n",
    "\n",
    "##### 1. Term frequency (count)\n",
    "##### 2. Term frequency-inverse document frequency (tfidf)\n",
    "\n",
    "### Supported clustering algorithms:\n",
    "\n",
    "##### 1. K-means (kmeans)\n",
    "##### 2. Density-Based Spatial Clustering of Applications with Noise (dbscan)\n",
    "##### 3. Meanshift (meanshift)\n",
    "\n",
    "### Supported dimensionality reduction algorithms:\n",
    "\n",
    "##### 1. Principal component analysis (pca)\n",
    "##### 2. t-distributed stochastic neighbor embedding (tsne)\n",
    "##### 3. Non-negative matrix factorization (nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5dd4673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package texthero:\n",
      "\n",
      "NAME\n",
      "    texthero - Texthero: python toolkit for text preprocessing, representation and visualization.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __about__\n",
      "    clustering\n",
      "    preprocessing\n",
      "    representation\n",
      "    statistics\n",
      "    texthero\n",
      "    version\n",
      "    visualization\n",
      "\n",
      "DATA\n",
      "    stopwords = <WordListCorpusReader in 'C:\\\\Users\\\\SWARNAVA\\\\AppData\\\\Ro...\n",
      "\n",
      "VERSION\n",
      "    1.0.5\n",
      "\n",
      "FILE\n",
      "    c:\\users\\swarnava\\anaconda3\\envs\\nlp3\\lib\\site-packages\\texthero\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import texthero\n",
    "help(texthero)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864fe15a",
   "metadata": {},
   "source": [
    "#### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ba09415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "text=\"It's a pleasant   day at Kolkata; at / (10:30) am\"\n",
    "series=pd.Series(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e4b3ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    It's a pleasant   day at Kolkata; at / (10:30) am\n",
       "dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3493c6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    It's a pleasant   day at Kolkata; at / (:) am\n",
       "dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import texthero as hero\n",
    "hero.preprocessing.remove_digits(series, only_blocks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a07a1d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    It s a pleasant   day at Kolkata  at    10 30  am\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Remove punctuations\n",
    "hero.remove_punctuation(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66f04f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    It's a pleasant   day at Kolkata; at /  am\n",
       "dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Remove Brackets\n",
    "hero.remove_brackets(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10a53582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    It's a pleasant day at Kolkata; at / (10:30) am\n",
       "dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hero.remove_whitespace(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9508a60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    pleasant day kolkata 10 30\n",
       "dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hero.clean(series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcff1ac5",
   "metadata": {},
   "source": [
    "https://texthero.org/docs/api/texthero.representation.tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae5e5d4",
   "metadata": {},
   "source": [
    "## EvalML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e672b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>SIX chances to win CASH! From 100 to 20,000 po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "1     spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "2     spam  WINNER!! As a valued network customer you have...\n",
       "3     spam  Had your mobile 11 months or more? U R entitle...\n",
       "4     spam  SIX chances to win CASH! From 100 to 20,000 po..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "\n",
    "input_data = urlopen('https://featurelabs-static.s3.amazonaws.com/spam_text_messages_modified.csv')\n",
    "data = pd.read_csv(input_data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "51e48120",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Independent And Dependent Features\n",
    "X=data.drop('Category',axis=1)\n",
    "y=data['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "55f69527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SIX chances to win CASH! From 100 to 20,000 po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Message\n",
       "0  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "1  FreeMsg Hey there darling it's been 3 week's n...\n",
       "2  WINNER!! As a valued network customer you have...\n",
       "3  Had your mobile 11 months or more? U R entitle...\n",
       "4  SIX chances to win CASH! From 100 to 20,000 po..."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0afb4e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.750084\n",
       "spam    0.249916\n",
       "Name: Category, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "13caedf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evalml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8ffe059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TRain A\\nd test data split\n",
    "X_train,X_test,y_train,y_test=evalml.preprocessing.split_data(X,y,problem_type='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7252955a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ProblemTypes.BINARY: 'binary'>,\n",
       " <ProblemTypes.MULTICLASS: 'multiclass'>,\n",
       " <ProblemTypes.REGRESSION: 'regression'>,\n",
       " <ProblemTypes.TIME_SERIES_REGRESSION: 'time series regression'>,\n",
       " <ProblemTypes.TIME_SERIES_BINARY: 'time series binary'>,\n",
       " <ProblemTypes.TIME_SERIES_MULTICLASS: 'time series multiclass'>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalml.problem_types.ProblemTypes.all_problem_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f212b811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>Haha I heard that, text me when you're around</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>I'm thinking that chennai forgot to come for a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>Can you tell Shola to please go to college of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2054</th>\n",
       "      <td>K k pa Had your lunch aha.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>staff.science.nus.edu.sg/~phyhcmk/teaching/pc1323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Message\n",
       "562       Haha I heard that, text me when you're around\n",
       "1253  I'm thinking that chennai forgot to come for a...\n",
       "1816  Can you tell Shola to please go to college of ...\n",
       "2054                         K k pa Had your lunch aha.\n",
       "511   staff.science.nus.edu.sg/~phyhcmk/teaching/pc1323"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1ad73619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalml import AutoMLSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b6951890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating pipelines to search over...\n",
      "8 pipelines ready for search.\n"
     ]
    }
   ],
   "source": [
    "automl=AutoMLSearch(X_train=X_train,y_train=y_train,problem_type='binary',max_batches=1,optimize_thresholds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6491e0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*****************************\n",
      "* Beginning pipeline search *\n",
      "*****************************\n",
      "\n",
      "Optimizing for Log Loss Binary. \n",
      "Lower score is better.\n",
      "\n",
      "Using SequentialEngine to train and score pipelines.\n",
      "Searching up to 1 batches for a total of 9 pipelines. \n",
      "Allowed model families: xgboost, decision_tree, extra_trees, random_forest, catboost, linear_model, lightgbm\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80662a2865c04940bef4278ac05c49d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'mode': 'lines+markers',\n",
       "              'name': 'Best Score',\n",
       "              'type'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline Pipeline: Mode Baseline Binary Classification Pipeline\n",
      "Mode Baseline Binary Classification Pipeline:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 8.638\n",
      "\n",
      "*****************************\n",
      "* Evaluating Batch Number 1 *\n",
      "*****************************\n",
      "\n",
      "Elastic Net Classifier w/ Text Featurization Component + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.243\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tElastic Net Classifier w/ Text Featurization Component + Standard Scaler may not perform as estimated on unseen data.\n",
      "Decision Tree Classifier w/ Text Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.802\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tDecision Tree Classifier w/ Text Featurization Component may not perform as estimated on unseen data.\n",
      "Random Forest Classifier w/ Text Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.155\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tRandom Forest Classifier w/ Text Featurization Component may not perform as estimated on unseen data.\n",
      "LightGBM Classifier w/ Text Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.215\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tLightGBM Classifier w/ Text Featurization Component may not perform as estimated on unseen data.\n",
      "Logistic Regression Classifier w/ Text Featurization Component + Standard Scaler:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.214\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tLogistic Regression Classifier w/ Text Featurization Component + Standard Scaler may not perform as estimated on unseen data.\n",
      "XGBoost Classifier w/ Text Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.179\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tXGBoost Classifier w/ Text Featurization Component may not perform as estimated on unseen data.\n",
      "Extra Trees Classifier w/ Text Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.252\n",
      "\tHigh coefficient of variation (cv >= 0.2) within cross validation scores.\n",
      "\tExtra Trees Classifier w/ Text Featurization Component may not perform as estimated on unseen data.\n",
      "CatBoost Classifier w/ Text Featurization Component:\n",
      "\tStarting cross validation\n",
      "\tFinished cross validation - mean Log Loss Binary: 0.526\n",
      "\n",
      "Search finished after 07:19            \n",
      "Best pipeline: Random Forest Classifier w/ Text Featurization Component\n",
      "Best pipeline Log Loss Binary: 0.154849\n"
     ]
    }
   ],
   "source": [
    "automl.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f99b0777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pipeline_name</th>\n",
       "      <th>mean_cv_score</th>\n",
       "      <th>standard_deviation_cv_score</th>\n",
       "      <th>validation_score</th>\n",
       "      <th>percent_better_than_baseline</th>\n",
       "      <th>high_variance_cv</th>\n",
       "      <th>parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Random Forest Classifier w/ Text Featurization...</td>\n",
       "      <td>0.154849</td>\n",
       "      <td>0.050536</td>\n",
       "      <td>0.110302</td>\n",
       "      <td>98.207418</td>\n",
       "      <td>True</td>\n",
       "      <td>{'Random Forest Classifier': {'n_estimators': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>XGBoost Classifier w/ Text Featurization Compo...</td>\n",
       "      <td>0.178639</td>\n",
       "      <td>0.063117</td>\n",
       "      <td>0.113254</td>\n",
       "      <td>97.932010</td>\n",
       "      <td>True</td>\n",
       "      <td>{'XGBoost Classifier': {'eta': 0.1, 'max_depth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Logistic Regression Classifier w/ Text Featuri...</td>\n",
       "      <td>0.214011</td>\n",
       "      <td>0.051316</td>\n",
       "      <td>0.165624</td>\n",
       "      <td>97.522538</td>\n",
       "      <td>True</td>\n",
       "      <td>{'Logistic Regression Classifier': {'penalty':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>LightGBM Classifier w/ Text Featurization Comp...</td>\n",
       "      <td>0.214580</td>\n",
       "      <td>0.072715</td>\n",
       "      <td>0.136260</td>\n",
       "      <td>97.515944</td>\n",
       "      <td>True</td>\n",
       "      <td>{'LightGBM Classifier': {'boosting_type': 'gbd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Elastic Net Classifier w/ Text Featurization C...</td>\n",
       "      <td>0.243362</td>\n",
       "      <td>0.075438</td>\n",
       "      <td>0.157467</td>\n",
       "      <td>97.182755</td>\n",
       "      <td>True</td>\n",
       "      <td>{'Elastic Net Classifier': {'alpha': 0.0001, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>Extra Trees Classifier w/ Text Featurization C...</td>\n",
       "      <td>0.252206</td>\n",
       "      <td>0.059094</td>\n",
       "      <td>0.216198</td>\n",
       "      <td>97.080377</td>\n",
       "      <td>True</td>\n",
       "      <td>{'Extra Trees Classifier': {'n_estimators': 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>CatBoost Classifier w/ Text Featurization Comp...</td>\n",
       "      <td>0.526403</td>\n",
       "      <td>0.015739</td>\n",
       "      <td>0.512717</td>\n",
       "      <td>93.906174</td>\n",
       "      <td>False</td>\n",
       "      <td>{'CatBoost Classifier': {'n_estimators': 10, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>Decision Tree Classifier w/ Text Featurization...</td>\n",
       "      <td>0.801766</td>\n",
       "      <td>0.213655</td>\n",
       "      <td>0.555179</td>\n",
       "      <td>90.718481</td>\n",
       "      <td>True</td>\n",
       "      <td>{'Decision Tree Classifier': {'criterion': 'gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>Mode Baseline Binary Classification Pipeline</td>\n",
       "      <td>8.638305</td>\n",
       "      <td>0.025020</td>\n",
       "      <td>8.623860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>{'Baseline Classifier': {'strategy': 'mode'}}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                      pipeline_name  mean_cv_score  \\\n",
       "0   3  Random Forest Classifier w/ Text Featurization...       0.154849   \n",
       "1   6  XGBoost Classifier w/ Text Featurization Compo...       0.178639   \n",
       "2   5  Logistic Regression Classifier w/ Text Featuri...       0.214011   \n",
       "3   4  LightGBM Classifier w/ Text Featurization Comp...       0.214580   \n",
       "4   1  Elastic Net Classifier w/ Text Featurization C...       0.243362   \n",
       "5   7  Extra Trees Classifier w/ Text Featurization C...       0.252206   \n",
       "6   8  CatBoost Classifier w/ Text Featurization Comp...       0.526403   \n",
       "7   2  Decision Tree Classifier w/ Text Featurization...       0.801766   \n",
       "8   0       Mode Baseline Binary Classification Pipeline       8.638305   \n",
       "\n",
       "   standard_deviation_cv_score  validation_score  \\\n",
       "0                     0.050536          0.110302   \n",
       "1                     0.063117          0.113254   \n",
       "2                     0.051316          0.165624   \n",
       "3                     0.072715          0.136260   \n",
       "4                     0.075438          0.157467   \n",
       "5                     0.059094          0.216198   \n",
       "6                     0.015739          0.512717   \n",
       "7                     0.213655          0.555179   \n",
       "8                     0.025020          8.623860   \n",
       "\n",
       "   percent_better_than_baseline  high_variance_cv  \\\n",
       "0                     98.207418              True   \n",
       "1                     97.932010              True   \n",
       "2                     97.522538              True   \n",
       "3                     97.515944              True   \n",
       "4                     97.182755              True   \n",
       "5                     97.080377              True   \n",
       "6                     93.906174             False   \n",
       "7                     90.718481              True   \n",
       "8                      0.000000             False   \n",
       "\n",
       "                                          parameters  \n",
       "0  {'Random Forest Classifier': {'n_estimators': ...  \n",
       "1  {'XGBoost Classifier': {'eta': 0.1, 'max_depth...  \n",
       "2  {'Logistic Regression Classifier': {'penalty':...  \n",
       "3  {'LightGBM Classifier': {'boosting_type': 'gbd...  \n",
       "4  {'Elastic Net Classifier': {'alpha': 0.0001, '...  \n",
       "5  {'Extra Trees Classifier': {'n_estimators': 10...  \n",
       "6  {'CatBoost Classifier': {'n_estimators': 10, '...  \n",
       "7  {'Decision Tree Classifier': {'criterion': 'gi...  \n",
       "8      {'Baseline Classifier': {'strategy': 'mode'}}  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl.rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "068f0ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline = BinaryClassificationPipeline(component_graph=[TextFeaturizer, RandomForestClassifier], parameters={'Random Forest Classifier':{'n_estimators': 100, 'max_depth': 6, 'n_jobs': -1}}, custom_hyperparameters={'None':{'sampling_ratio': 0.25}}, random_seed=0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl.best_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7cf4b6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipeline = automl.best_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e613547d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************************************************************\n",
      "* Random Forest Classifier w/ Text Featurization Component *\n",
      "************************************************************\n",
      "\n",
      "Problem Type: binary\n",
      "Model Family: Random Forest\n",
      "\n",
      "Pipeline Steps\n",
      "==============\n",
      "1. Text Featurization Component\n",
      "2. Random Forest Classifier\n",
      "\t * n_estimators : 100\n",
      "\t * max_depth : 6\n",
      "\t * n_jobs : -1\n",
      "\n",
      "Training\n",
      "========\n",
      "Training for binary problems.\n",
      "Total training time (including CV): 49.2 seconds\n",
      "\n",
      "Cross Validation\n",
      "----------------\n",
      "             Log Loss Binary  MCC Binary   AUC  Precision    F1  Balanced Accuracy Binary  Accuracy Binary  Sensitivity at Low Alert Rates # Training # Validation\n",
      "0                      0.110       0.895 0.987      0.938 0.921                     0.942            0.961                           0.246      1,594          797\n",
      "1                      0.144       0.854 0.980      0.919 0.888                     0.917            0.946                           0.246      1,594          797\n",
      "2                      0.210       0.783 0.962      0.839 0.837                     0.891            0.918                           0.266      1,594          797\n",
      "mean                   0.155       0.844 0.977      0.899 0.882                     0.917            0.942                           0.252          -            -\n",
      "std                    0.051       0.057 0.013      0.052 0.042                     0.026            0.022                           0.011          -            -\n",
      "coef of var            0.326       0.067 0.013      0.058 0.048                     0.028            0.023                           0.045          -            -\n"
     ]
    }
   ],
   "source": [
    "automl.describe_pipeline(automl.rankings.iloc[0][\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d4f1c08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluate on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ad6357df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Binary: 0.9732441471571907\n"
     ]
    }
   ],
   "source": [
    "scores = best_pipeline.score(X_test, y_test,  objectives=evalml.objectives.get_core_objectives('binary'))\n",
    "print(f'Accuracy Binary: {scores[\"Accuracy Binary\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
